# non-ordinal categorical variables. Ordinal categorical variables are encoded using
# one hot encoding.
non_ordinal(agency_data_used$account_type)
# Reusable function for non-ordingal
non_ordinal <- function(the_var){
the_var <- as.integer(the_var)
}
# Convert all the categorical features to numeric. Simple translation is required for
# non-ordinal categorical variables. Ordinal categorical variables are encoded using
# one hot encoding.
non_ordinal(agency_data_used$account_type)
# Read the original data set
agency_data_orig <- read.xlsx('./data/AgencyData_clean.xlsx', sheetIndex=1, stringsAsFactors=T)
# Trim the phat - i.e., data that's irrelevant
agency_data_used <- agency_data_orig[c(-1, -4)]
str(agency_data_used)
# Reusable function for one-hot encodigin
ordinal <- function(the_var){
}
# Reusable function for non-ordingal
non_ordinal <- function(the_var){
the_var <- as.integer(the_var)
}
# Convert all the categorical features to numeric. Simple translation is required for
# non-ordinal categorical variables. Ordinal categorical variables are encoded using
# one hot encoding.
non_ordinal(agency_data_used$account_type)
View(agency_data_used)
# Reusable function for non-ordingal
non_ordinal <- function(the_var){
the_var <- as.integer(the_var)
return (the_var)
}
# Convert all the categorical features to numeric. Simple translation is required for
# non-ordinal categorical variables. Ordinal categorical variables are encoded using
# one hot encoding.
agency_data_used$account_type <- non_ordinal(agency_data_used$account_type)
View(agency_data_used)
View(agency_data_used)
agency_data_used$assigned_agent <- non_ordinal(agency_data_used$assigned_agent)
agency_data_used$lob <- non_ordinal(agency_data_used$lob)
dmy <- dummyVars(" ~ policy_term", data = customers)
library(caret)
dmy <- dummyVars(" ~ policy_term", data = customers)
dmy <- dummyVars(" ~ policy_term", data = agency_data_used)
View(dmy)
trsf <- data.frame(predict(dmy, newdata = agency_data_used))
View(agency_data_used)
rbind(as.matrix(agency_data_used), as.matrix(trsf))
bind_rows(agency_data_used, trsf)
library(dplyr)
bind_rows(agency_data_used, trsf)
View(agency_data_used)
View(trsf)
foobar <- bind_rows(agency_data_used, trsf)
View(foobar)
typeof(trsf$policy_term.12.Months)
str(trsf)
foobar <- cbind(agency_data_used, trsf)
View(foobar)
agency_data_used <- cbind(agency_data_used, trsf)
View(agency_data_used)
agency_data_used$policy_term <- NULL
View(agency_data_used)
# Trim the phat - i.e., data that's irrelevant
agency_data_used <- agency_data_orig[c(-1, -4)]
str(agency_data_used)
# Reusable function for one-hot encodigin
ordinal <- function(the_var, the_var_str){
dmy <- dummyVars(paste(" ~ ", the_var_str), data = agency_data_used)
trsf <- data.frame(predict(dmy, newdata = agency_data_used))
agency_data_used <- cbind(agency_data_used, trsf)
agency_data_used$the_var <- NULL
return(agency_data_used)
}
# Reusable function for non-ordingal
non_ordinal <- function(the_var){
the_var <- as.integer(the_var)
return (the_var)
}
agency_data_used <- ordinal(agency_data_used$policy_term, "policy_term")
View(agency_data_used)
# Trim the phat - i.e., data that's irrelevant
agency_data_used <- agency_data_orig[c(-1, -4)]
str(agency_data_used)
agency_data_used <- ordinal(agency_data_used$policy_term, "policy_term")
View(agency_data_used)
# Reusable function for one-hot encodigin
ordinal <- function(the_var, the_var_str){
dmy <- dummyVars(paste(" ~ ", the_var_str), data = agency_data_used)
trsf <- data.frame(predict(dmy, newdata = agency_data_used))
agency_data_used <- cbind(agency_data_used, trsf)
the_var <- NULL
return(agency_data_used)
}
# Trim the phat - i.e., data that's irrelevant
agency_data_used <- agency_data_orig[c(-1, -4)]
agency_data_used <- ordinal(agency_data_used$policy_term, "policy_term")
# Reusable function for one-hot encodigin
ordinal <- function(the_var, the_var_str){
dmy <- dummyVars(paste(" ~ ", the_var_str), data = agency_data_used)
dummy_df <- data.frame(predict(dmy, newdata = agency_data_used))
agency_data_used <- cbind(agency_data_used, dummy_df)
agency_data_used$the_var_str <- NULL
return(agency_data_used)
}
# Trim the phat - i.e., data that's irrelevant
agency_data_used <- agency_data_orig[c(-1, -4)]
agency_data_used <- ordinal(agency_data_used$policy_term, "policy_term")
View(agency_data_used)
# Reusable function for one-hot encodigin
ordinal <- function(the_var, the_var_str){
dmy <- dummyVars(paste(" ~ ", the_var_str), data = agency_data_used)
dummy_df <- data.frame(predict(dmy, newdata = agency_data_used))
agency_data_used <- cbind(agency_data_used, dummy_df)
return(agency_data_used)
}
# Reusable function for one-hot encodigin
ordinal <- function(the_var, the_var_str){
dmy <- dummyVars(paste(" ~ ", the_var_str), data = agency_data_used)
dummy_df <- data.frame(predict(dmy, newdata = agency_data_used))
agency_data_used <- cbind(agency_data_used, dummy_df)
return(agency_data_used)
}
# Reusable function for one-hot encodigin
ordinal <- function(the_var, the_var_str){
dmy <- dummyVars(paste(" ~ ", the_var_str), data = agency_data_used)
dummy_df <- data.frame(predict(dmy, newdata = agency_data_used))
agency_data_used <- cbind(agency_data_used, dummy_df)
the_var <- NULL
return(agency_data_used)
}
# Trim the phat - i.e., data that's irrelevant
agency_data_used <- agency_data_orig[c(-1, -4)]
agency_data_used <- ordinal(agency_data_used$policy_term, "policy_term")
View(agency_data_used)
# Reusable function for one-hot encodigin
ordinal <- function(the_var, the_var_str){
dmy <- dummyVars(paste(" ~ ", the_var_str), data = agency_data_used)
dummy_df <- data.frame(predict(dmy, newdata = agency_data_used))
agency_data_used <- cbind(agency_data_used, dummy_df)
agency_data_used$the_var_str <- NULL
return(agency_data_used)
}
# Trim the phat - i.e., data that's irrelevant
agency_data_used <- agency_data_orig[c(-1, -4)]
agency_data_used <- ordinal(agency_data_used$policy_term, "policy_term")
View(agency_data_used)
agency_data_used$policy_term <- NULL
View(agency_data_used)
agency_data_used$rating_state <- non_ordinal(agency_data_used$rating_state)
View(agency_data_used)
source('~/GitHub/agency_data/linear_regression.R')
View(train_ad)
source('~/GitHub/agency_data/linear_regression.R')
View(agency_data_used)
View(agency_data_used)
fit <- glm(transaction_type ~ ., data=agency_data_used, family=poisson())
summary(fit)
library(gbm)
install.packages("gbm")
install.package("xgboost")
install.packages("xgboost")
library(gbm)
library(xgboost)
# Read the original data set
agency_data_orig <- read.xlsx('./data/AgencyData_clean.xlsx', sheetIndex=1, stringsAsFactors=T)
# Trim the phat - i.e., data that's irrelevant
agency_data_used <- agency_data_orig[c(-1, -4)]
# for reproducibility
set.seed(123)
# train GBM model
gbm.fit <- gbm(
formula = transaction_type ~ .,
distribution = "gaussian",
data = agency_data_used,
n.trees = 10000,
interaction.depth = 1,
shrinkage = 0.001,
cv.folds = 5,
n.cores = NULL, # will use all cores by default
verbose = FALSE
)
agency_data_used$effective_date <- as.Date.numeric(agency_data_used$effective_date)
agency_data_used$effective_date <- as.numeric(agency_data_used$effective_date)
View(agency_data_used)
# train GBM model
gbm.fit <- gbm(
formula = transaction_type ~ .,
distribution = "gaussian",
data = agency_data_used,
n.trees = 10000,
interaction.depth = 1,
shrinkage = 0.001,
cv.folds = 5,
n.cores = NULL, # will use all cores by default
verbose = FALSE
)
(gbm.fit)
View(gbm.fit)
# get MSE and compute RMSE
sqrt(min(gbm.fit$cv.error))
# plot loss function as a result of n trees added to the ensemble
gbm.perf(gbm.fit, method = "cv")
summary(fit)
library(xlsx)
library(caret)
library(dplyr)
# Read the original data set
agency_data_orig <- read.xlsx('./data/AgencyData_clean.xlsx', sheetIndex=1, stringsAsFactors=T)
# Trim the phat - i.e., data that's irrelevant
agency_data_used <- agency_data_orig[c(-1, -4)]
str(agency_data_used)
# Reusable function for one-hot encoding
ordinal <- function(the_var, the_var_str){
dmy <- dummyVars(paste(" ~ ", the_var_str), data = agency_data_used)
dummy_df <- data.frame(predict(dmy, newdata = agency_data_used))
agency_data_used <- cbind(agency_data_used, dummy_df)
return(agency_data_used)
}
# Reusable function for non-ordingal
non_ordinal <- function(the_var){
the_var <- as.integer(the_var)
return (the_var)
}
nor <-function(x) { (x -min(x))/(max(x)-min(x)) }
# Convert date to numeric
agency_data_used$effective_date <- as.numeric(agency_data_used$effective_date)
# Convert all the categorical features to numeric. Simple translation is required for
# non-ordinal categorical variables. Ordinal categorical variables are encoded using
# one hot encoding.
agency_data_used$account_type <- non_ordinal(agency_data_used$account_type)
agency_data_used$assigned_agent <- non_ordinal(agency_data_used$assigned_agent)
agency_data_used$lob <- non_ordinal(agency_data_used$lob)
agency_data_used$master_company <- non_ordinal(agency_data_used$master_company)
agency_data_used$policy_type <- non_ordinal(agency_data_used$policy_type)
agency_data_used <- ordinal(agency_data_used$policy_term, "policy_term")
agency_data_used$policy_term <- NULL # can't get this into the function for some reason
agency_data_used$rating_state <- non_ordinal(agency_data_used$rating_state)
agency_data_used$status <- non_ordinal(agency_data_used$status)
agency_data_used$transaction_type <- non_ordinal(agency_data_used$transaction_type)
agency_data_used_norm <- as.data.frame(lapply(agency_data_used[,c(1,2,3,4, 5, 6, 7, 8,
9, 10, 11)], nor))
summary(agency_data_norm)
summary(agency_data_used_norm)
histogram(account_type)
histogram(agency_data_used_norm$account_type)
histogram(agency_data_used_norm$assigned_agent)
View(agency_data_used_norm)
# Setup train and test data, 70% / 30%
train_ad <- sample_frac(agency_data_used, 0.7)
sid <- as.numeric(rownames(train_ad)) # because rownames() returns character
test_ad <- agency_data_used[-sid,]
agency_data_used_norm <- as.data.frame(lapply(agency_data_used[,c(1,2,3,4, 5, 6, 7, 8,
9, 10)], nor))
View(agency_data_used_norm)
set.seed(123)
train_ad_target <- agency_data_used.subset[11]
train_ad_target <- agency_data_used.subset[11]
train_ad_target <- agency_data_used_norm[train_ad, 11]
test_ad_target <- agency_data_used_norm[-train_ad, 11]
train_ad_target <- agency_data_used[train_ad, 11]
train_ad_target <- agency_data_used[11]
sample_norm <- sample(1:nrow(agency_data_used_norm),
size=nrow(agency_data_used_norm)*0.7,replace = FALSE)
train_ad_target <- agency_data_used[sample_norm, 11]
test_ad_target <- agency_data_used[-sample_norm, 11]
library(class)
NROW(train_ad_target)
sqrt(NROW(train_ad_target))
knn.40 <- knn(train=train_ad, test=test_ad, cl=train_ad_target, k=40)
knn.41 <- knn(train=train_ad, test=test_ad, cl=train_ad_target, k=41)
ACC.40 <- 100 * sum(test_ad_target == knn.40)/NROW(test_ad_target)
ACC.41 <- 100 * sum(test_ad_target == knn.41)/NROW(test_ad_target)
print("K-40 accuracy:", ACC.40)
print(paste("K-40 accuracy:", ACC.40))
print(paste("k=41 accurac:", ACC.41))
table(knn.40 ,test.test_ad_target)
table(knn.40 ,test_ad_target)
table(knn.41 ,test_ad_target)
confusionMatrix(table(knn.40 ,test_ad_target))
library(caret)
confusionMatrix(table(knn.40 ,test_ad_target))
confusionMatrix(table(knn.40, test_ad_target))
confusionMatrix(knn.40)
confusionMatrix(table(knn.40, test_ad_target))
confusionMatrix(table(knn.40, test_ad_target))
View(test_ad)
set.seed(123)
# Setup train and test data, 70% / 30%
train_ad <- sample_frac(agency_data_used_norm, 0.7)
sid <- as.numeric(rownames(train_ad)) # because rownames() returns character
test_ad <- agency_data_used_norm[-sid,]
sample_norm <- sample(1:nrow(agency_data_used_norm),
size=nrow(agency_data_used_norm)*0.7,replace = FALSE)
train_ad_target <- agency_data_used[sample_norm, 11]
test_ad_target <- agency_data_used[-sample_norm, 11]
sqrt(NROW(train_ad_target))
sample_norm <- sample(1:nrow(agency_data_used_norm),
size=nrow(agency_data_used_norm)*0.7,replace = FALSE)
train_ad <- agency_data_used[sample_norm,]
test_ad <- agency_data_used(-sample_norm,)
test_ad <- agency_data_used[-sample_norm,]
train_ad <- agency_data_used[sample_norm,]
train_ad_target <- agency_data_used[sample_norm, 11]
test_ad_target <- agency_data_used[-sample_norm, 11]
sqrt(NROW(train_ad_target))
knn.40 <- knn(train=train_ad, test=test_ad, cl=train_ad_target, k=40)
knn.41 <- knn(train=train_ad, test=test_ad, cl=train_ad_target, k=41)
ACC.40 <- 100 * sum(test_ad_target == knn.40)/NROW(test_ad_target)
ACC.41 <- 100 * sum(test_ad_target == knn.41)/NROW(test_ad_target)
print(paste("K=40 accuracy:", ACC.40))
print(paste("k=41 accurac:", ACC.41))
table(knn.40 ,test_ad_target)
table(knn.41 ,test_ad_target)
confusionMatrix(table(knn.40, test_ad_target))
set.seed(123)
sample_norm <- sample(1:nrow(agency_data_used_norm),
size=nrow(agency_data_used_norm)*0.7,replace = FALSE)
train_ad <- agency_data_used[sample_norm,]
test_ad <- agency_data_used[-sample_norm,]
train_ad_target <- agency_data_used[sample_norm, 11]
test_ad_target <- agency_data_used[-sample_norm, 11]
sqrt(NROW(train_ad_target))
knn.40 <- knn(train=train_ad, test=test_ad, cl=train_ad_target, k=40)
knn.41 <- knn(train=train_ad, test=test_ad, cl=train_ad_target, k=41)
ACC.40 <- 100 * sum(test_ad_target == knn.40)/NROW(test_ad_target)
ACC.41 <- 100 * sum(test_ad_target == knn.41)/NROW(test_ad_target)
print(paste("K=40 accuracy:", ACC.40))
print(paste("k=41 accurac:", ACC.41))
table(knn.40 ,test_ad_target)
table(knn.41 ,test_ad_target)
confusionMatrix(table(knn.40, test_ad_target))
#confusionMatrix(table(knn.40, test_ad_target))
i=1
k.optm=1
for (i in 1:28){
knn.mod <- knn(train=train_ad, test=test_ad, cl=train_ad_target, k=i)
k.optm[i] <- 100 * sum(test_ad_target == knn.mod)/NROW(test_ad_target)
k=i
cat(k,'=',k.optm[i],'')
}
plot(k.optm, type="b", xlab="K- Value",ylab="Accuracy level")
source('~/GitHub/agency_data/gradient_boost_machine.R')
View(gbm.fit)
# plot loss function as a result of n trees added to the ensemble
gbm.perf(gbm.fit, method = "cv")
# get MSE and compute RMSE
sqrt(min(gbm.fit$cv.error))
# print results
print(gbm.fit)
# get MSE and compute RMSE
min_MSE <- which.min(gbm.fit2$cv.error)
# get MSE and compute RMSE
sqrt(min(gbm.fit$cv.error))
set.seed(123)
# train GBM model
gbm.fit2 <- gbm(
formula = transaction_type ~ .,
distribution = "gaussian",
data = agency_data_used,
n.trees = 5000,
interaction.depth = 3,
shrinkage = 0.1,
cv.folds = 5,
n.cores = NULL, # will use all cores by default
verbose = FALSE
)
min_MSE <- which.min(gbm.fit2$cv.error)
sqrt(gbm.fit2$cv.error[min_MSE])
gbm.perf(gbm.fit2, method = "cv")
library(randomForest)
install.packages('randomForest')
library(randomForest)
View(agency_data_used)
# Read the original data set
agency_data_orig <- read.xlsx('./data/AgencyData_clean.xlsx', sheetIndex=1, stringsAsFactors=T)
# Trim the phat - i.e., data that's irrelevant
agency_data_used <- agency_data_orig[c(-1, -4)]
View(agency_data_used)
str(agency_data_used)
set.seed(100)
sample_ad <- sample(nrow(agency_data_used), 0.7*nrow(agency_data_used), replace = FALSE)
train_ad <- agency_data_used[sample_ad,]
test_ad <- agency_data_used[-sample_ad,]
rf_model <- randomForest(transaction_type ~ ., data = train_ad, importance = TRUE)
View(train_ad)
View(test_ad)
View(train_ad)
rf_model <- randomForest(transaction_type ~ ., data = train_ad, importance = TRUE)
str(agency_data_orig)
View(agency_data_orig)
foo <- is.na(agency_data_orig$transaction_type)
agency_data_orig[complete.cases(agency_data_orig),]
foo <- is.na(agency_data_orig$transaction_type)
na.omit(agency_data_orig)
foo <- is.na(agency_data_orig$transaction_type)
ad_drop <- na.omit(agency_data_orig)
na.omit(agency_data_orig)
na.omit(agency_data_orig$transaction_type)
foo <- is.na(agency_data_orig$transaction_type)
df %>% drop_na()
library(tidyr)
install.packages('tidyr')
install.packages("tidyr")
library(tidyr)
library(tidyr)
library(tidyr)
install.packages('tidyr')
library(tidyr)
df %>% drop_na()
View(fit)
df %>% drop_na(agency_data_orig)
View(agency_data_used)
foo <- which(is.na(agency_data_orig$transaction_type))
foo <- which(is.na(agency_data_used$transaction_type))
rf_model <- randomForest(transaction_type ~ ., data = train_ad, importance = TRUE)
library(xlsx)
library(caret)
library(randomForest)
# Read the original data set
agency_data_orig <- read.xlsx('./data/AgencyData_clean.xlsx', sheetIndex=1, stringsAsFactors=T)
foo <- which(is.na(agency_data_orig$transaction_type))
str(agency_data_orig)
# Trim the phat - i.e., data that's irrelevant
agency_data_used <- agency_data_orig[c(-1, -4)]
str(agency_data_used)
foo <- which(is.na(agency_data_used$transaction_type))
set.seed(100)
sample_ad <- sample(nrow(agency_data_used), 0.7*nrow(agency_data_used), replace = FALSE)
train_ad <- agency_data_used[sample_ad,]
test_ad <- agency_data_used[-sample_ad,]
rf_model <- randomForest(transaction_type ~ ., data = train_ad, importance = TRUE)
form <- formula("transaction_type ~ .")
rf_model <- randomForest(form, data = train_ad, importance = TRUE)
set.seed(100)
sample_ad <- sample(nrow(agency_data_used), 0.7*nrow(agency_data_used), replace = FALSE)
train_ad <- agency_data_used[sample_ad,]
train_ad$transaction_type <- factor(train_ad$transaction_type)
test_ad <- agency_data_used[-sample_ad,]
test_ad$transaction_type <- factor(test_ad$transaction_type)
rf_model <- randomForest(transaction_type ~ ., data = train_ad, importance = TRUE)
rf_model
rf_model <- randomForest(transaction_type ~ ., data = train_ad, ntree=1000, mtry=6, importance = TRUE)
rf_model
rf_model <- randomForest(transaction_type ~ ., data = train_ad, ntree=500, importance = TRUE)
rf_model
rf_model <- randomForest(transaction_type ~ ., data = train_ad, ntree=500, importance = TRUE, do.trace=100)
rf_model <- randomForest(transaction_type ~ ., data = train_ad, ntree=600, importance = TRUE, do.trace=100)
rf_model <- randomForest(transaction_type ~ ., data = train_ad, ntree=700, importance = TRUE, do.trace=100)
rf_model
rf_model <- randomForest(transaction_type ~ ., data = train_ad, ntree=700, importance = TRUE, do.trace=100)
source('~/GitHub/agency_data/random_forest_model.R')
source('~/GitHub/agency_data/linear_regression.R')
summary(fit)
fit_premium <- glm(written_premium ~ ., data=agency_data_used, family=poisson())
summary(fit_premium)
warnings()
fit_premium <- glm(round(written_premium, 0) ~ ., data=agency_data_used, family=poisson())
summary(fit_premium)
histogram(agency_data_used$written_premium)
# Transform written_premium to normal distribution
agency_data_used$written_premium <- sqrt(agency_data_used$written_premium)
histogram(agency_data_used$written_premium)
# Transform written_premium to normal distribution
agency_data_used$written_premium <- agency_data_used$written_premium^2
histogram(agency_data_used$written_premium)
# Transform written_premium to normal distribution
agency_data_used$written_premium <- 1/agency_data_used$written_premium
histogram(agency_data_used$written_premium)
source('~/GitHub/agency_data/linear_regression.R')
summary(fit_premium)
# Transform written_premium to normal distribution
foo <- agency_data_used$written_premium^2
#agency_data_used$written_premium <- agency_data_used$written_premium
#agency_data_used$written_premium <- sqrt(agency_data_used$written_premium)
histogram(foo)
# Transform written_premium to normal distribution
foo <- log10(agency_data_used$written_premium)
#agency_data_used$written_premium <- agency_data_used$written_premium
#agency_data_used$written_premium <- sqrt(agency_data_used$written_premium)
histogram(foo)
# Transform written_premium to normal distribution
agency_data_used$written_premium  <- log10(agency_data_used$written_premium)
histogram(agency_data_used$written_premium)
fit_premium <- glm(round(written_premium, 0) ~ ., data=agency_data_used, family=poisson())
summary(fit_premium)
fit_premium <- glm(round(written_premium, 0) ~ ., data=agency_data_used, family=gaussian())
summary(fit_premium)
source('~/GitHub/agency_data/linear_regression.R')
fit <- glm(transaction_type ~ ., data=agency_data_used, family=poisson())
summary(fit)
fit_premium <- glm(round(written_premium, 0) ~ ., data=agency_data_used, family=poisson())
summary(fit_premium)
# Transform written_premium to normal distribution
#agency_data_used$written_premium  <- log10(agency_data_used$written_premium)
#histogram(agency_data_used$written_premium)
fit_premium <- glm(round(written_premium, 0) ~ ., data=agency_data_used, family=gaussian())
summary(fit_premium)
library(xlsx)
library(caret)
library(dplyr)
library(class)
source('./data/load_trim_data.R')
source('./data/load_trim_data.R')
source('./data/load_trim_data.R')
source('load_trim_data.R')
source('~/GitHub/agency_data/knn_model.R')
source('~/GitHub/agency_data/knn_model.R')
confusionMatrix(table(knn.40, test_ad_target))
source('~/GitHub/agency_data/generalized_liner_model.R')
summary(fit)
