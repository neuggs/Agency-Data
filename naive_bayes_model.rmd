---
title: "Naive Bayes Agency Data Predictive Model"
author: "Frank Neugebauer"
date: "9/17/2019"
output:
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(xlsx)
library(dplyr)
library(e1071)
library(caret)
library(formattable)
```

# Agency Data Simple Naive Bayes Predictive Model

Agency data intuitively has value to an agency and potentially to an agent's carrier partner(s). One way to gain value from that data - which exists within agency management systems - is to create one or more predictive models. Gaining insight from predictive modeling using agency data is the fundamental nature of this effort.

This project's objectives are to:

* Understand **if** a predictive model can be created
* Determine the relative value of such a predictive model to an agency
* Explore other methods aside from Naive Bayes for further investigation

The data itself is contained within a codebook, the output of which can be found within the `data` folder of this project. There is also a small data preparation file in the same location.

**Epilogue**
It's clear that predictive modelling with agency has value. The contents of this analysis are a single perspective of such an effort, although subsequently, at least one other perspective is presented to give the reader a glimpse into how varied this analysis could potentially be.

This uses a largely unaltered simple Naive Bayes model to set a baseline for subsequent predictive models, which can be created using other algorithms.

## Data Load and Trim

The first step is to load the full data set. The source for this R code is found within `load_trip_data.R`.

The `account_name` is not important in this case, although if the transaction had been more sequential (i.e., from new business through renwal), the name would have matter very much. This is an important consideration for using this with other data. Similarly, the `branch_name` was just the name of the agency and every observation had the same value. If there had been multiple agencies (or branch locations), this feature would have been very important.

```{r}
# Read the original data set
#agency_data_orig <- read.xlsx('./data/AgencyData_clean.xlsx', sheetIndex=1, stringsAsFactors=T)
source('load_trim_data.R')

```

## Split Data

With the irrelevant attributes identified, the following gets rid of those and then show the data structure and separate the `train` and `test` data sets. 


```{r}

# Setup train and test data, 70% / 30%
train_ad <- sample_frac(agency_data_used, 0.7)
sid <- as.numeric(rownames(train_ad)) # because rownames() returns character
test_ad <- agency_data_used[-sid,]
```

## Build the Model

This step is set aside because with a big data set, it would be slow or put into a distributed system. This model is using `transaction_type` as the target variable. In simple terms, the model is making predictions for the various transaction types given other conditions, such as the line of business.

The transactions, which are the basic policy lifecycle transactions across lines of business, are:

1. New Business - when a client is first getting a policy (this assumes one or more quotes have been provided)
2. Policy Change - any type of change to a policy, including fixing spellings through endoresements for additional coverage
3. Cancel - there are three versions of this
  + Conf - a confirmed cancellation by the insured
  + Req - a requested cancellation by the insured
  + DNR - a cancellation caused by a non-renewal
4. Renew - when a policy term ends, it goes into a renewal state, which has multiple conditions:
  + Just renew, meaning the policy renewed, which can be automatic in many situations
  + Renewal Quote - this trigger premium calculations for the renewal
  + Renewal Re-Quote - often time, the insured will ask for different options, which results in a re-quote (which can occur multiple times)
5. Non-Renewal - this means the policy term end is the end of the policy altogether and happen when an insurance company no longer wants to cover the insured
6. Re-issue - this happens when a policy is cancelled but then "uncancelled," a situation most often due to payment problems (that can be the fault of the insurance company) - there is normally a lapse in coverate for a re-issue
7. Re-instate - like re-issue, a re-instatement happens when a cancelled policy is renewed, but occurs with no lapse in coverage
8. Re-write - an insurance policy is a contract and requires endorsements to make even simple changes; if a policy has anything major that requires significant changes, the policy is cancelled and then a re-write is issued with the correction(s)

The situations where predicting which transaction will occur is important. For example, can the model predict the likelihood of a renewal for a personal auto policy (it can go deeper than that, but it's a good illustration of the principle).

As previously stated, a Naive Bayes model is used. For a quick description of Naive Bayes, please refer to [this nice Wikipedia description.](https://en.wikipedia.org/wiki/Naive_Bayes_classifier)


```{r}
nb_model <- naiveBayes(transaction_type~., data=train_ad)
```

## Conditional Probabilities

The model creates the conditional probabilities based on the training data. This output shows those, which is interesting unto themselves

Intuitively, it may appear that this is simple math - e.g., if there were 100 transactions and 40 of them were renewals, then the probability should be 40%. However, this model is multi-factored, which means a more accurate way of considering it is, given the line of business was personal auto, there's some non-obvious probability that a renwal will occur. That's the kind of probabilities being reported here.

For the sake of readability, the bulk of the contitional probabilities can be found in the corresponding `.rmd` and `.pdf` files within the `data` directory.

```{r}
nb_model$tables$policy_term
nb_model$tables$policy_type
# create one for binned premium
nb_model$tables$rating_state
```

Some important points about the conditional probabilities:

1. The conditional probabilities by `assigned_agent` and transaction can help in workforce capacity planning. For example, if some agents are handling renewals more frequently, perhaps those agents need help.

2. `master_company` data can be used similarly to how `assigned_agent` is used in order to ensure companies with the most valuable business, or most frequently number of transactions, have the greatest amount of help.

3. For `policy_term` there is something unexpected: virtually all `New Business` transactions are 12 month, whereas renewals are 76%/24% 12 to 6 month. Why?

 
## Make Predictions

As with the model, this step can take some real time with a large data set, so it's segregated from processing / output.


```{r}
nb_predict <- predict(nb_model, test_ad)
```

## Show the Predictions

This is similar to the conditional probability, but applies it to show the actual predicted values. The points about conditional probabilities all apply for the predictions, but this is where number of transactions are shown. As such, this is a more valuable workforce capacity planning metric.

**However**, the numbers shown here are based on the size of the test set. If you want to actually simulate workforce capacity, you'll need a sample that is similar to what you'll expect in a year (a model like this one can be used to make that prediction).


```{r}
table(nb_predict, test_ad$account_type)
table(nb_predict, test_ad$assigned_agent)
table(nb_predict, test_ad$lob)
table(nb_predict, test_ad$master_company)
#table(nb_predict, test_ad$effective_date) # bin this
table(nb_predict, test_ad$policy_term)
table(nb_predict, test_ad$policy_type)
table(nb_predict, test_ad$rating_state)
table(nb_predict, test_ad$status)
```

## Confusion Matrix

Build a simple confusion matrix and then plot it. Despite its name, there's no confusion within a confusion matrix, which is really more about understanding the error rate for statistical classification (which is what this model does). In fact, the confusion matrix is also called an error matrix.

For more, you can read [this Wikipedia description of confusion matrix.](https://en.wikipedia.org/wiki/Confusion_matrix)

```{r}
confusion_matrix <- table(nb_predict, test_ad$transaction_type)
```

## Finally, Show Stats on the Confusion Matrix

```{r echo=FALSE}
con_matrix <- confusionMatrix(confusion_matrix)
con_matrix
accuracy <- percent(con_matrix[["overall"]][["Accuracy"]])
lower_ci <- percent(con_matrix[["overall"]][["AccuracyLower"]])
upper_ci <- percent(con_matrix[["overall"]][["AccuracyUpper"]])
```

## Results Interpretation 
The predictive quality for agency data shows promise. Some very specific points about the results:

1. A `r accuracy` accuracy for `transaction_type` is very good. While it doesn't guarantee anything, this value is baseline for the usefulness of the model. This model should be checked over time against reality to see if it remains valid.

2. There's a 95% chance that the accuracy is between `r lower_ci` and `r upper_ci` - also very good. The name - confidence iterval - is well done and indicates how likely the accuracy is going to be correct. Do **not* take this to mean there are guarantees - save one: the model will make the wrong predictions sometimes, but the right predictions far more often.

3. The accuracy is greater than the No Information Rate, which indicates significance. (Not always, but the upper CI is.)

It's also very much worth noting that if you run this model over and over, you'll get slightly different (albeit similar) results. This is because the predictions vary since the underlying logic re-runs the entire set of logic (including separating the test and train data). This is normal and expected, but don't just pick the best results as **the** results - the best results are just **a** result.

## Using the Model - Out of Sample
When you know all the data and are making predictions, that's useful only to understand the viability of the model. The application of the model is for data that's new so you can understand (more) about what's going to happen. That's a true prediction. To do that, a dummy record is created (which could be a real record or some guesses at what you'll have over the next week, month, etc.) and then fed into the model.


```{r}
new_data = data.frame(
  'account_type' = 'Personal',
  'assigned_agent' = 'Walter Doyle',
  'lob' = 'Auto (Personal)',
  'master_company' = 'Beaulah Insurance',
  'effective_date' = as.Date('2014-07-29'),
  'policy_term' = '12 Months',
  'policy_type' = 'Personal',
  'annual_premium' = 1234.33,
  'written_premium' = 993.23,
  'rating_state' = 'FL',
  'status' = 'Active'
)

single_model <- predict(nb_model, newdata=new_data)
table(single_model)

```

The transaction with the `1` is what the model predicted the transaction would be. You can do this with large sets of data to understand (for example) how many underwriters are required in Florida.

## Areas for Improvement
While the model is quite accurate, it can still be improved. For example, the premium values are skewed and could be transformed using `log10` (I did this just to see what would happen and the distribution becomes almost normal). The target variable can also be changed, which will likely yield a different view of the data.

## References
[1] O'Neil, Cathy and Schutt, Rachel. (2014). Doing Data Science. Sabastopol, CA. O'Reilly Media.

[2] Lander, Jared P. (2017). R for Everyone: Advanced Analytics and Graphics. Hoboken, NJ. Pearson Education as Addison-Wesley.

[3] James, Gareth, Witten, Daniela, Hastie, Trevor, and Tibshirani, Robert. (2015). New York, NY. Springer.

[4] Abbott, Dean. (2014). Applied Predictive Analytics: Principles and Techniques for the Professional Data Analyst. Indianapolis, IN. John Wiley & Sons.

